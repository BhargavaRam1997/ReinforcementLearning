{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym\n",
    "\n",
    "from statistics import median, mean\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Each of these is its own game.\n",
    "for episode in range(5):\n",
    "    env.reset()\n",
    "    # this is each frame, up to 200...but we wont make it that far.\n",
    "    for t in range(400):\n",
    "        # This will display the environment\n",
    "        # Only display if you really want to see it.\n",
    "        # Takes much longer to display it.            \n",
    "        env.render()\n",
    "        \n",
    "        # This will just create a sample action in any environment.\n",
    "        # In this environment, the action can be 0 or 1, which is left or right\n",
    "        action = env.action_space.sample()\n",
    "            \n",
    "        # this executes the environment with an action, \n",
    "        # and returns the observation of the environment, \n",
    "        # the reward, if the env is over, and other info.\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # OBS: [Position of cart, angle of pole, derivtive of position, derivative of angle].\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_games = 10000\n",
    "goal_steps = 500\n",
    "score_requirement = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_games():\n",
    "    # [OBS, MOVES]\n",
    "    training_data = []\n",
    "    # all scores:\n",
    "    scores = []\n",
    "    # just the scores that met our threshold:\n",
    "    accepted_scores = []\n",
    "    # iterate through however many games we want:\n",
    "    for _ in range(initial_games):\n",
    "        score = 0\n",
    "        # moves specifically from this environment:\n",
    "        game_memory = []\n",
    "        # previous observation that we saw\n",
    "        prev_observation = []\n",
    "        # for each frame in 200\n",
    "        for _ in range(goal_steps):\n",
    "            # choose random action (0 or 1)\n",
    "            action = random.randrange(0,2)\n",
    "            # do it!\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            # notice that the observation is returned FROM the action\n",
    "            # so we'll store the previous observation here, pairing\n",
    "            # the prev observation to the action we'll take.\n",
    "            # We pair the both the action and observation at time t. \n",
    "            if len(prev_observation) > 0 :\n",
    "                game_memory.append([prev_observation, action])\n",
    "            prev_observation = observation\n",
    "            score+=reward\n",
    "            if done: break\n",
    "\n",
    "        # IF our score is higher than our threshold, we'd like to save\n",
    "        # every move we made\n",
    "        # NOTE the reinforcement methodology here. \n",
    "        # all we're doing is reinforcing the score, we're not trying \n",
    "        # to influence the machine in any way as to HOW that score is \n",
    "        # reached.\n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                # convert to one-hot (this is the output layer for our neural network)\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0]\n",
    "                    \n",
    "                # saving our training data [OBS, One-Hot-Encoded action]\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "        # reset env to play again\n",
    "        env.reset()\n",
    "        # save overall scores\n",
    "        scores.append(score)\n",
    "    \n",
    "    # just in case you wanted to reference later\n",
    "    training_data_save = np.array(training_data)\n",
    "    np.save('saved.npy',training_data_save)\n",
    "    \n",
    "    # some stats here, to further illustrate the neural network magic!\n",
    "    print('Average accepted score:', mean(accepted_scores))\n",
    "    print('Median score for accepted scores:',median(accepted_scores))\n",
    "    print(Counter(accepted_scores))\n",
    "    \n",
    "    return training_data, Counter(accepted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 62.07297297297297\n",
      "Median score for accepted scores: 58.0\n",
      "Counter({50.0: 38, 52.0: 26, 53.0: 26, 51.0: 20, 54.0: 19, 56.0: 18, 57.0: 16, 65.0: 14, 55.0: 13, 59.0: 13, 58.0: 12, 64.0: 11, 69.0: 9, 61.0: 9, 60.0: 8, 66.0: 8, 72.0: 8, 67.0: 8, 62.0: 8, 70.0: 7, 68.0: 7, 76.0: 7, 63.0: 6, 71.0: 5, 79.0: 5, 82.0: 4, 81.0: 4, 75.0: 3, 84.0: 3, 83.0: 3, 74.0: 3, 89.0: 3, 73.0: 3, 80.0: 3, 77.0: 2, 90.0: 2, 97.0: 2, 86.0: 2, 93.0: 2, 101.0: 1, 126.0: 1, 91.0: 1, 99.0: 1, 88.0: 1, 95.0: 1, 78.0: 1, 94.0: 1, 96.0: 1, 87.0: 1})\n"
     ]
    }
   ],
   "source": [
    "training_data, accepted_scores = generate_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 48 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.bar(accepted_scores.keys(), accepted_scores.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=[1, 4], activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1, 128)            640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 256)            33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 128)            32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 2)              258       \n",
      "=================================================================\n",
      "Total params: 66,818\n",
      "Trainable params: 66,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor([tf.expand_dims(d[0], 0) for d in training_data], dtype=tf.float32)\n",
    "y = tf.convert_to_tensor([i[1] for i in training_data], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:20000]\n",
    "y = y[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X, y)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[ 0.03783184,  0.15936837, -0.04896386, -0.34330508]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, target = next(iter(train_ds))\n",
    "feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.48698846 0.5130115 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(model(tf.expand_dims(feature[0], 0), training=True).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.6716 - accuracy: 0.6000\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.6887 - accuracy: 0.5760\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.6816 - accuracy: 0.5820\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.6742 - accuracy: 0.5700\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.6644 - accuracy: 0.6380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1369d76d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=5, steps_per_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.8965654 , 0.10343464]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model(tf.expand_dims(feature[0], 0))\n",
    "model.predict(tf.expand_dims(feature[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 0.0208274 ,  0.22764194,  0.02892191, -0.2985553 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "tf.cast(tf.reshape(obs, [1, 1, -1]), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game in range(10):\n",
    "    score = 0\n",
    "    prev_obs = []\n",
    "    env.reset()\n",
    "    for _ in range(500):\n",
    "        env.render()\n",
    "        # Since we don't have a previous observation we have no choice\n",
    "        # but to intially make a random action.\n",
    "        if len(prev_obs) == 0:\n",
    "            action = random.randrange(0, 2)\n",
    "        else:\n",
    "            prepared_obs = tf.cast(tf.reshape(prev_obs, [1, 1, -1]), tf.float32)\n",
    "            # Note: output is onehot encoded vector [0.43, 0.57]. Hence 'Right'.\n",
    "            action = np.argmax(model(prepared_obs))\n",
    "        # Get observation, rewards etc... from the next state with respect to the action\n",
    "        # our model has predicted.\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        score += reward\n",
    "        \n",
    "        if done: break\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
